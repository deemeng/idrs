{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding (Not Yet)\n",
    ">1. time steps: NF, NB in Backward / Forward \\\n",
    "Sizes of forwards and backwards memories in the BRNN. If you want a\n",
    "standard feed-forward NN instead (no recursion, just a window) set to 0.\n",
    "For a BRNN I imagine that the 20-30 range should be appropriate as a\n",
    "starting point.\n",
    "\n",
    ">2. Hidden units in Backward / Forward \\\n",
    "NHF,NHB:\n",
    "Number of hidden units in the forwards and backwards memory networks.\n",
    "This should typically be a little larger than NF,NB. For an FFNN set\n",
    "both to 0 just like NF and NB.\n",
    "\n",
    ">3. CoF, CoB \\\n",
    "Control the number of forwards and backwards memories that are seen by\n",
    "the output network. These will be (2 x CoX + 1). For a BRNN 3-4 should\n",
    "be OK. For a FFNN set both to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option_BRNN file\n",
    "epsilon 0.002 \\\n",
    "NLayers 1\\\n",
    "NU\t21\\\n",
    "NHo\t9\\\n",
    "NHt\t20\\\n",
    "Ht\t15\\\n",
    "NHi\t15\\\n",
    "Hi\t9\n",
    "\n",
    "gamma\t   7\\\n",
    "context 0\n",
    "\n",
    "Classes 2\\\n",
    "Thresholds 0.5\n",
    "\n",
    "DEEP\t0\n",
    "\n",
    "seed    949\\\n",
    "shuffle 1\\\n",
    "batch_blocks 1800\\\n",
    "readModel    0\\\n",
    "readEpoch    0\\\n",
    "nEpochs      5000\\\n",
    "adaptive     10000\\\n",
    "reload       1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# processing bar\n",
    "from tqdm import tqdm\n",
    "# import hyperparameters\n",
    "from cnn_hyperparams import *\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f'{model_name}board_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.root.setLevel(logging.INFO)\n",
    "# logging.basicConfig(level=logging.NOTSET)\n",
    "logging.basicConfig(filename=log_name+'_test', \n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s %(message)s', \n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %P',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "> Note: \\\n",
    "the reason not using tensor to save protein Sequences and Targets is we have varying length sequences! \\\n",
    "solve this problem we could consider pading. But our dataset lens range from about 20 to 10,000. Thus, padding is not a good idea here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fpath):\n",
    "    num_protein = 0\n",
    "    num_i = 0\n",
    "    num_o = 0\n",
    "\n",
    "    # p_names = []\n",
    "    p_lens = []\n",
    "    # p_seqs = []\n",
    "    # p_anns = []\n",
    "    p_data = []\n",
    "    with open(fpath) as fp:\n",
    "        num_protein = int(fp.readline())\n",
    "        num_io = fp.readline().split(' ')\n",
    "        num_i = int(num_io[0])\n",
    "        num_o = int(num_io[0])\n",
    "\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            # p_name = line[:-1]\n",
    "            p_len = int(fp.readline())\n",
    "            p_sequence = torch.tensor([int(x) for x in fp.readline().split(' ')], \n",
    "                                      dtype=torch.float32).reshape(-1, 21)\n",
    "            p_annotation = torch.tensor([int(x) for x in fp.readline().split(' ')], dtype=torch.float32)\n",
    "            # skip empty\n",
    "            next(fp)\n",
    "            # p_names.append(p_name)\n",
    "            p_lens.append(p_len)\n",
    "            # p_seqs.append(p_sequence)\n",
    "            # p_anns.append(p_annotation)\n",
    "            p_data.append([p_sequence, p_annotation])\n",
    "            line = fp.readline()    \n",
    "    return p_data, p_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_plens = load_dataset(train_fpath)\n",
    "test_data, test_plens = load_dataset(test_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ori = train_data.copy()\n",
    "test_data_ori = test_data.copy()\n",
    "\n",
    "train_data = train_data_ori[:15]\n",
    "test_data = test_data_ori[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(p_data)\n",
    "n_samples = len(train_plens)\n",
    "# train_data = p_data[:int(n_samples*0.85)]\n",
    "# test_data = p_data[int(n_samples*0.85):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(train_data)\n",
    "n_test = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_class(input_data):\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "    for batch_idx, (data, target) in enumerate(input_data):\n",
    "        c0 = c0 + len(target) - target.sum()\n",
    "        c1 = c1 + target.sum()\n",
    "    return c0, c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train c0, c1: 6779.0, 514.0\n",
      "test c0, c1: 3478.0, 292.0\n"
     ]
    }
   ],
   "source": [
    "train_c0, train_c1 = get_num_class(train_data)\n",
    "test_c0, test_c1 = get_num_class(test_data)\n",
    "print(f'train c0, c1: {train_c0}, {train_c1}')\n",
    "print(f'test c0, c1: {test_c0}, {test_c1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocPlot(train_label, train_probs, val_label, val_probs):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    fpr, tpr, thresholds = roc_curve(train_label, train_probs)\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=model_name+\" ROC curve (area = %0.2f)\" % roc_auc,\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Training\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(val_label, val_probs)\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=model_name+\" ROC curve (area = %0.2f)\" % roc_auc,\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Validation\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "> kernel size = len*21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, in_channels, kernel_size_row, \n",
    "                 kernel_size_col, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 21),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        self.conv12 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 1),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        self.conv13 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 1),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        self.conv14 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 1),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        \n",
    "#         self.conv12 = nn.Conv2d(in_channels=7, out_channels=1, \n",
    "#                                kernel_size=(3, 3),\n",
    "#                               stride = 1, padding=(1, 1))\n",
    "        \n",
    "        self.conv5 = nn.Conv1d(in_channels=1, out_channels=1,\n",
    "                              kernel_size=1, stride=1, \n",
    "                               padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.relu(self.conv13(x))\n",
    "        x = F.relu(self.conv14(x))\n",
    "        x = self.conv5(x.squeeze(3))\n",
    "        x = self.sigmoid(x)\n",
    "        # x = (x - x.min())/(x.max()-x.min())\n",
    "        return x\n",
    "    \n",
    "    def train_batch(self, train_data, optimizer, criterion):\n",
    "        # set training state to model\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        with tqdm(total=num_batch, position=0) as progress_bar:\n",
    "            for i in range(num_batch):\n",
    "                # get current batch dataset\n",
    "                bat_data = train_data[i*batch_size:min((i+1)*batch_size, n_samples)]\n",
    "                bat_loss = 0\n",
    "                for batch_idx, (data, target) in enumerate(bat_data):\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "                    # forward\n",
    "                    scores = self(data.reshape(1, 1, data.shape[0], data.shape[1]))\n",
    "                    loss = criterion(scores.squeeze(0).squeeze(0), target)\n",
    "\n",
    "                    # ERROR\n",
    "                    bat_loss = bat_loss + loss\n",
    "                # backword\n",
    "                optimizer.zero_grad()\n",
    "                bat_loss.backward()\n",
    "                #gradient descent or adam step\n",
    "                optimizer.step()\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "    def val_batch(self, val_data, criterion): \n",
    "        # set evaluation state to the model\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        losses = []\n",
    "        \n",
    "        class_probs = []\n",
    "        class_label = []\n",
    "        \n",
    "        # no gradient needed\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_data):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                # forward\n",
    "                scores = self(data.reshape(1, 1, data.shape[0], data.shape[1]))\n",
    "                loss = criterion(scores.squeeze(0).squeeze(0), target)\n",
    "                # ERROR\n",
    "                losses.append(loss.cpu()) # loss for each batch\n",
    "                # save for ploting curve\n",
    "                class_probs.append(scores.squeeze(0).squeeze(0).cpu())\n",
    "                class_label.append(target.cpu())\n",
    "                \n",
    "        # overall loss\n",
    "        loss = np.mean(losses)\n",
    "        return loss, class_probs, class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(input_size, in_channels, kernel_size_row, \n",
    "                 kernel_size_col, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3 gates for GRU\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name)\n",
    "#     print(param.size())\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_sum = 0 \n",
    "for batch_idx, (data, target) in enumerate(train_data):\n",
    "    dis_sum = dis_sum + target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([dis_sum/sum(train_plens), 1-dis_sum/sum(train_plens)], dtype=torch.float)\n",
    "\n",
    "# class_weights = torch.tensor([1, 18], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.7862e-05, 9.9993e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_ALLOC_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_169503/2552162322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# acc & loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_169503/2047769417.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, train_data, optimizer, criterion)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_169503/2047769417.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_ALLOC_FAILED"
     ]
    }
   ],
   "source": [
    "logging.warning(f\"model_name: {model_name} \\n \\\n",
    "                batch_size: {batch_size} \\n \\\n",
    "                num_epochs: {num_epochs} \\n \\\n",
    "                learning_rate: {learning_rate}\")\n",
    "'''\n",
    "For each epoch:\n",
    "    1. training\n",
    "    2. calculate acc & loss for training dataset\n",
    "    3. calculate acc & loss for val dataset\n",
    "    4. add accs & losses to tensorboard\n",
    "    5. finding the best val epoch\n",
    "'''\n",
    "batch_size = 2\n",
    "# shuffled_data = p_data.copy()\n",
    "num_batch = (n_train+batch_size-1)//batch_size\n",
    "\n",
    "# val_loss = []\n",
    "num_epochs = 2\n",
    "plot_nEpoch = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logging.warning('*********************************')\n",
    "    logging.warning(f'epoch: {epoch}')\n",
    "    logging.warning('*********************************')\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "    ################\n",
    "    # 1. Training\n",
    "    ################\n",
    "    \n",
    "    plot_pr_roc = (epoch % plot_nEpoch == 0)\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    # training\n",
    "    model.train_batch(train_data, optimizer, criterion)\n",
    "    \n",
    "    # acc & loss\n",
    "    train_loss, t_class_probs, t_class_label = model.val_batch(train_data, criterion)\n",
    "    val_loss, v_class_probs, v_class_label = model.val_batch(test_data, criterion)\n",
    "    # val_accs.append(val_acc)\n",
    "    \n",
    "    print('Training: Loss:')\n",
    "    print(train_loss)\n",
    "    \n",
    "    print('VAL: Loss:')\n",
    "    print(val_loss)\n",
    "    \n",
    "    logging.warning('Training:')\n",
    "    logging.warning(f'Loss: {train_loss}')\n",
    "\n",
    "    logging.warning('Validation:')\n",
    "    logging.warning(f'Loss: {val_loss}')\n",
    "    \n",
    "    ##\n",
    "    # for tensorboard plots\n",
    "    ##\n",
    "    writer.add_scalar(\"TRAIN: loss\", train_loss, epoch)\n",
    "    #writer.add_scalar(\"TRAIN: acc\", train_acc, epoch)\n",
    "    \n",
    "    writer.add_scalar(\"VAL: loss\", val_loss, epoch)\n",
    "    #writer.add_scalar(\"VAL: acc\", val_acc, epoch)\n",
    "    \n",
    "    ##\n",
    "    # PR-curve & ROC-curve\n",
    "    ##\n",
    "    if plot_pr_roc:\n",
    "        train_probs = torch.cat([batch for batch in t_class_probs])\n",
    "        train_label = torch.cat(t_class_label)\n",
    "\n",
    "        writer.add_pr_curve(f'TRAIN: pr_curve e{epoch}', train_label, train_probs, 0)\n",
    "\n",
    "        val_probs = torch.cat([batch for batch in v_class_probs])\n",
    "        val_label = torch.cat(v_class_label)\n",
    "\n",
    "        writer.add_pr_curve(f'VAL: pr_curve e{epoch}', val_label, val_probs, 0)\n",
    "\n",
    "        roc_fig = rocPlot(train_label, train_probs, val_label, val_probs)\n",
    "        writer.add_figure(f'Train vs VAL: roc_curve e{epoch}', roc_fig)\n",
    "    ##\n",
    "    # Save model every m epochs\n",
    "    ##\n",
    "    if epoch % checkpoint_m == 0:\n",
    "        cPATH = f\"checkpoint/{model_name}_{epoch}.pth\"\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': train_loss,\n",
    "                    }, cPATH)\n",
    "        \n",
    "logging.warning(f\"model_name: {model_name} \\n \\\n",
    "                batch_size: {batch_size} \\n \\\n",
    "                num_epochs: {num_epochs} \\n \\\n",
    "                learning_rate: {learning_rate} \\n \")\n",
    "                # best_val_epoch: {int(np.argmax(val_accs)+1)}\") # the best val epoch\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 1, 997, 1]             316\n",
      "            Conv2d-2            [-1, 1, 997, 1]              16\n",
      "            Conv1d-3               [-1, 1, 997]               2\n",
      "           Sigmoid-4               [-1, 1, 997]               0\n",
      "================================================================\n",
      "Total params: 334\n",
      "Trainable params: 334\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "seq_len = 997\n",
    "summary(model, (1, seq_len, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y.max(1) returns two tensors…\n",
    "\n",
    "**1. the max value in each row of y** \\\n",
    " 0.7822\\\n",
    " 0.5563\\\n",
    " 0.9425\\\n",
    "**2. the column index at which the max value is found.**\\\n",
    " 3\\\n",
    " 3\\\n",
    " 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mPATH = '/home/dimeng/code_Di/python_code/test_model.pth'\n",
    "torch.save(model.state_dict(), mPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cPATH = '/home/dimeng/code_Di/python_code/test_checkpoint.pth'\n",
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            ...\n",
    "            }, cPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(input_size, in_channels, kernel_size_row, \n",
    "                 kernel_size_col, num_classes)\n",
    "model.load_state_dict(torch.load(mPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CNN(\n",
       "  (conv1): Conv2d(1, 21, kernel_size=(7, 21), stride=(1, 1), padding=(3, 0))\n",
       "  (conv2): Conv1d(21, 21, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (conv3): Conv1d(21, 2, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       ")>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
