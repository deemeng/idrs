{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding (Not Yet)\n",
    ">1. time steps: NF, NB in Backward / Forward \\\n",
    "Sizes of forwards and backwards memories in the BRNN. If you want a\n",
    "standard feed-forward NN instead (no recursion, just a window) set to 0.\n",
    "For a BRNN I imagine that the 20-30 range should be appropriate as a\n",
    "starting point.\n",
    "\n",
    ">2. Hidden units in Backward / Forward \\\n",
    "NHF,NHB:\n",
    "Number of hidden units in the forwards and backwards memory networks.\n",
    "This should typically be a little larger than NF,NB. For an FFNN set\n",
    "both to 0 just like NF and NB.\n",
    "\n",
    ">3. CoF, CoB \\\n",
    "Control the number of forwards and backwards memories that are seen by\n",
    "the output network. These will be (2 x CoX + 1). For a BRNN 3-4 should\n",
    "be OK. For a FFNN set both to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option_BRNN file\n",
    "epsilon 0.002 \\\n",
    "NLayers 1\\\n",
    "NU\t21\\\n",
    "NHo\t9\\\n",
    "NHt\t20\\\n",
    "Ht\t15\\\n",
    "NHi\t15\\\n",
    "Hi\t9\n",
    "\n",
    "gamma\t   7\\\n",
    "context 0\n",
    "\n",
    "Classes 2\\\n",
    "Thresholds 0.5\n",
    "\n",
    "DEEP\t0\n",
    "\n",
    "seed    949\\\n",
    "shuffle 1\\\n",
    "batch_blocks 1800\\\n",
    "readModel    0\\\n",
    "readEpoch    0\\\n",
    "nEpochs      5000\\\n",
    "adaptive     10000\\\n",
    "reload       1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# processing bar\n",
    "from tqdm import tqdm\n",
    "# import hyperparameters\n",
    "from cnn_hyperparams import *\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f'{model_name}board_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.root.setLevel(logging.INFO)\n",
    "# logging.basicConfig(level=logging.NOTSET)\n",
    "logging.basicConfig(filename=log_name+'_test', \n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s %(message)s', \n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %P',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "> Note: \\\n",
    "the reason not using tensor to save protein Sequences and Targets is we have varying length sequences! \\\n",
    "solve this problem we could consider pading. But our dataset lens range from about 20 to 10,000. Thus, padding is not a good idea here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fpath):\n",
    "    num_protein = 0\n",
    "    num_i = 0\n",
    "    num_o = 0\n",
    "\n",
    "    # p_names = []\n",
    "    p_lens = []\n",
    "    # p_seqs = []\n",
    "    # p_anns = []\n",
    "    p_data = []\n",
    "    with open(fpath) as fp:\n",
    "        num_protein = int(fp.readline())\n",
    "        num_io = fp.readline().split(' ')\n",
    "        num_i = int(num_io[0])\n",
    "        num_o = int(num_io[0])\n",
    "\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            # p_name = line[:-1]\n",
    "            p_len = int(fp.readline())\n",
    "            p_sequence = torch.tensor([int(x) for x in fp.readline().split(' ')], \n",
    "                                      dtype=torch.float32).reshape(-1, 21)\n",
    "            p_annotation = torch.tensor([int(x) for x in fp.readline().split(' ')], dtype=torch.float32)\n",
    "            # skip empty\n",
    "            next(fp)\n",
    "            # p_names.append(p_name)\n",
    "            p_lens.append(p_len)\n",
    "            # p_seqs.append(p_sequence)\n",
    "            # p_anns.append(p_annotation)\n",
    "            p_data.append([p_sequence, p_annotation])\n",
    "            line = fp.readline()    \n",
    "    return p_data, p_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_plens = load_dataset(train_fpath)\n",
    "test_data, test_plens = load_dataset(test_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ori = train_data.copy()\n",
    "test_data_ori = test_data.copy()\n",
    "\n",
    "train_data = train_data_ori[:15]\n",
    "test_data = test_data_ori[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(p_data)\n",
    "n_samples = len(train_plens)\n",
    "# train_data = p_data[:int(n_samples*0.85)]\n",
    "# test_data = p_data[int(n_samples*0.85):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(train_data)\n",
    "n_test = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_class(input_data):\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "    for batch_idx, (data, target) in enumerate(input_data):\n",
    "        c0 = c0 + len(target) - target.sum()\n",
    "        c1 = c1 + target.sum()\n",
    "    return c0, c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train c0, c1: 6779.0, 514.0\n",
      "test c0, c1: 3478.0, 292.0\n"
     ]
    }
   ],
   "source": [
    "train_c0, train_c1 = get_num_class(train_data)\n",
    "test_c0, test_c1 = get_num_class(test_data)\n",
    "print(f'train c0, c1: {train_c0}, {train_c1}')\n",
    "print(f'test c0, c1: {test_c0}, {test_c1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocPlot(train_label, train_probs, val_label, val_probs):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    fpr, tpr, thresholds = roc_curve(train_label, train_probs)\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=model_name+\" ROC curve (area = %0.2f)\" % roc_auc,\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Training\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(val_label, val_probs)\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=model_name+\" ROC curve (area = %0.2f)\" % roc_auc,\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Validation\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "> kernel size = len*21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, in_channels, kernel_size_row, \n",
    "                 kernel_size_col, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 21),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        self.conv12 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 1),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        self.conv13 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 1),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        self.conv14 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=(15, 1),\n",
    "                              stride = 1, padding=(7, 0))\n",
    "        \n",
    "#         self.conv12 = nn.Conv2d(in_channels=7, out_channels=1, \n",
    "#                                kernel_size=(3, 3),\n",
    "#                               stride = 1, padding=(1, 1))\n",
    "        \n",
    "        self.conv5 = nn.Conv1d(in_channels=1, out_channels=1,\n",
    "                              kernel_size=1, stride=1, \n",
    "                               padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.relu(self.conv13(x))\n",
    "        x = F.relu(self.conv14(x))\n",
    "        x = self.conv5(x.squeeze(3))\n",
    "        x = self.sigmoid(x)\n",
    "        # x = (x - x.min())/(x.max()-x.min())\n",
    "        return x\n",
    "    \n",
    "    def train_batch(self, train_data, optimizer, criterion):\n",
    "        # set training state to model\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        with tqdm(total=num_batch, position=0) as progress_bar:\n",
    "            for i in range(num_batch):\n",
    "                # get current batch dataset\n",
    "                bat_data = train_data[i*batch_size:min((i+1)*batch_size, n_samples)]\n",
    "                bat_loss = 0\n",
    "                for batch_idx, (data, target) in enumerate(bat_data):\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "                    # forward\n",
    "                    scores = self(data.reshape(1, 1, data.shape[0], data.shape[1]))\n",
    "                    loss = criterion(scores.squeeze(0).squeeze(0), target)\n",
    "\n",
    "                    # ERROR\n",
    "                    bat_loss = bat_loss + loss\n",
    "                # backword\n",
    "                optimizer.zero_grad()\n",
    "                bat_loss.backward()\n",
    "                #gradient descent or adam step\n",
    "                optimizer.step()\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "    def val_batch(self, val_data, criterion): \n",
    "        # set evaluation state to the model\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        losses = []\n",
    "        \n",
    "        class_probs = []\n",
    "        class_label = []\n",
    "        \n",
    "        # no gradient needed\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_data):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                # forward\n",
    "                scores = self(data.reshape(1, 1, data.shape[0], data.shape[1]))\n",
    "                loss = criterion(scores.squeeze(0).squeeze(0), target)\n",
    "                # ERROR\n",
    "                losses.append(loss.cpu()) # loss for each batch\n",
    "                # save for ploting curve\n",
    "                class_probs.append(scores.squeeze(0).squeeze(0).cpu())\n",
    "                class_label.append(target.cpu())\n",
    "                \n",
    "        # overall loss\n",
    "        loss = np.mean(losses)\n",
    "        return loss, class_probs, class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(input_size, in_channels, kernel_size_row, \n",
    "                 kernel_size_col, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv11.weight tensor([[[[ 0.0091, -0.0213, -0.0146, -0.0212,  0.0089,  0.0518,  0.0038,\n",
      "            0.0265,  0.0371,  0.0388,  0.0416, -0.0048,  0.0167, -0.0280,\n",
      "            0.0154, -0.0277,  0.0133,  0.0504,  0.0253,  0.0042,  0.0229],\n",
      "          [-0.0557,  0.0368,  0.0119, -0.0125,  0.0296,  0.0039,  0.0219,\n",
      "            0.0247, -0.0205, -0.0002, -0.0086,  0.0294,  0.0545, -0.0054,\n",
      "           -0.0342, -0.0216, -0.0330,  0.0277, -0.0323,  0.0292,  0.0500],\n",
      "          [-0.0319,  0.0315, -0.0187, -0.0342, -0.0028,  0.0257,  0.0151,\n",
      "           -0.0096,  0.0364, -0.0491,  0.0355, -0.0053, -0.0203, -0.0104,\n",
      "            0.0020, -0.0495, -0.0046,  0.0156,  0.0181, -0.0486, -0.0055],\n",
      "          [-0.0303, -0.0080, -0.0195, -0.0107,  0.0549,  0.0198,  0.0474,\n",
      "            0.0396, -0.0136, -0.0088,  0.0023,  0.0239, -0.0073, -0.0208,\n",
      "           -0.0164, -0.0169,  0.0026,  0.0204, -0.0278,  0.0320, -0.0256],\n",
      "          [ 0.0133, -0.0143, -0.0507,  0.0430,  0.0381, -0.0540, -0.0406,\n",
      "           -0.0357, -0.0217,  0.0315, -0.0071,  0.0539,  0.0291, -0.0076,\n",
      "            0.0408, -0.0396, -0.0023, -0.0440, -0.0472, -0.0029,  0.0368],\n",
      "          [-0.0322, -0.0404, -0.0289,  0.0079, -0.0268, -0.0180, -0.0008,\n",
      "           -0.0252,  0.0309,  0.0396,  0.0488,  0.0531, -0.0334,  0.0241,\n",
      "           -0.0410,  0.0223, -0.0048,  0.0344, -0.0306, -0.0028, -0.0274],\n",
      "          [-0.0473,  0.0303, -0.0281, -0.0111,  0.0032,  0.0060,  0.0085,\n",
      "           -0.0021, -0.0270,  0.0125,  0.0031, -0.0242, -0.0194, -0.0459,\n",
      "           -0.0061,  0.0347, -0.0182,  0.0012,  0.0464,  0.0231, -0.0206],\n",
      "          [ 0.0026, -0.0057,  0.0453,  0.0132,  0.0366, -0.0555, -0.0309,\n",
      "           -0.0253, -0.0048,  0.0236,  0.0318, -0.0270,  0.0553,  0.0058,\n",
      "            0.0203, -0.0502,  0.0264, -0.0514,  0.0150, -0.0546, -0.0022],\n",
      "          [-0.0233, -0.0320, -0.0382,  0.0199, -0.0166, -0.0400, -0.0183,\n",
      "            0.0465, -0.0230, -0.0219,  0.0088,  0.0405,  0.0095, -0.0405,\n",
      "           -0.0434, -0.0393, -0.0557, -0.0417,  0.0028,  0.0049, -0.0034],\n",
      "          [ 0.0552,  0.0456,  0.0060,  0.0202, -0.0524,  0.0036,  0.0281,\n",
      "            0.0424,  0.0486,  0.0138,  0.0316, -0.0409,  0.0504,  0.0500,\n",
      "           -0.0151,  0.0323, -0.0046, -0.0266, -0.0005, -0.0315,  0.0130],\n",
      "          [ 0.0099,  0.0301, -0.0554,  0.0318, -0.0481, -0.0427,  0.0483,\n",
      "           -0.0113,  0.0416, -0.0214,  0.0219,  0.0315,  0.0051, -0.0071,\n",
      "            0.0224, -0.0490, -0.0233, -0.0082,  0.0009,  0.0051, -0.0115],\n",
      "          [-0.0460,  0.0200, -0.0081,  0.0010, -0.0175, -0.0271,  0.0031,\n",
      "            0.0377,  0.0306, -0.0354, -0.0213,  0.0400,  0.0029, -0.0080,\n",
      "           -0.0222,  0.0167, -0.0483, -0.0013,  0.0393,  0.0188,  0.0273],\n",
      "          [-0.0275,  0.0292, -0.0318, -0.0263,  0.0091,  0.0164,  0.0510,\n",
      "            0.0104, -0.0060, -0.0312, -0.0276,  0.0103, -0.0132, -0.0285,\n",
      "           -0.0520,  0.0035, -0.0477, -0.0228, -0.0119, -0.0290, -0.0514],\n",
      "          [-0.0049,  0.0311, -0.0179,  0.0113, -0.0385, -0.0307,  0.0117,\n",
      "            0.0318,  0.0128,  0.0118, -0.0522, -0.0139, -0.0102,  0.0139,\n",
      "            0.0150, -0.0267,  0.0270, -0.0062,  0.0180,  0.0159,  0.0072],\n",
      "          [-0.0509,  0.0307,  0.0260, -0.0164,  0.0554, -0.0035, -0.0479,\n",
      "            0.0216, -0.0430,  0.0279, -0.0315, -0.0533,  0.0100,  0.0513,\n",
      "            0.0081,  0.0539, -0.0399,  0.0160, -0.0467,  0.0313, -0.0434]]]])\n",
      "conv11.bias tensor([0.0114])\n",
      "conv12.weight tensor([[[[ 0.0078],\n",
      "          [-0.0507],\n",
      "          [ 0.2573],\n",
      "          [ 0.1875],\n",
      "          [ 0.2462],\n",
      "          [ 0.0993],\n",
      "          [ 0.1448],\n",
      "          [ 0.0743],\n",
      "          [-0.0907],\n",
      "          [ 0.0071],\n",
      "          [-0.1216],\n",
      "          [ 0.1000],\n",
      "          [ 0.0760],\n",
      "          [ 0.1574],\n",
      "          [ 0.1734]]]])\n",
      "conv12.bias tensor([0.1716])\n",
      "conv13.weight tensor([[[[ 0.0952],\n",
      "          [-0.2508],\n",
      "          [ 0.2147],\n",
      "          [ 0.0347],\n",
      "          [-0.2392],\n",
      "          [-0.2540],\n",
      "          [ 0.1562],\n",
      "          [ 0.0579],\n",
      "          [-0.1777],\n",
      "          [-0.0356],\n",
      "          [-0.1305],\n",
      "          [ 0.2350],\n",
      "          [-0.1415],\n",
      "          [ 0.2158],\n",
      "          [-0.0244]]]])\n",
      "conv13.bias tensor([-0.0970])\n",
      "conv14.weight tensor([[[[ 0.1673],\n",
      "          [-0.1997],\n",
      "          [-0.2023],\n",
      "          [ 0.0964],\n",
      "          [ 0.1343],\n",
      "          [-0.2167],\n",
      "          [-0.0306],\n",
      "          [-0.2550],\n",
      "          [-0.0854],\n",
      "          [ 0.2560],\n",
      "          [-0.2240],\n",
      "          [-0.2014],\n",
      "          [-0.0953],\n",
      "          [ 0.1576],\n",
      "          [ 0.1650]]]])\n",
      "conv14.bias tensor([0.0011])\n",
      "conv5.weight tensor([[[-0.1016]]])\n",
      "conv5.bias tensor([0.8496])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3 gates for GRU\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name)\n",
    "#     print(param.size())\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_sum = 0 \n",
    "for batch_idx, (data, target) in enumerate(train_data):\n",
    "    dis_sum = dis_sum + target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([dis_sum/sum(train_plens), 1-dis_sum/sum(train_plens)], dtype=torch.float)\n",
    "\n",
    "# class_weights = torch.tensor([1, 18], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.7862e-05, 9.9993e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_ALLOC_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_169503/2552162322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# acc & loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_169503/2047769417.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, train_data, optimizer, criterion)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_169503/2047769417.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_ALLOC_FAILED"
     ]
    }
   ],
   "source": [
    "logging.warning(f\"model_name: {model_name} \\n \\\n",
    "                batch_size: {batch_size} \\n \\\n",
    "                num_epochs: {num_epochs} \\n \\\n",
    "                learning_rate: {learning_rate}\")\n",
    "'''\n",
    "For each epoch:\n",
    "    1. training\n",
    "    2. calculate acc & loss for training dataset\n",
    "    3. calculate acc & loss for val dataset\n",
    "    4. add accs & losses to tensorboard\n",
    "    5. finding the best val epoch\n",
    "'''\n",
    "batch_size = 2\n",
    "# shuffled_data = p_data.copy()\n",
    "num_batch = (n_train+batch_size-1)//batch_size\n",
    "\n",
    "# val_loss = []\n",
    "num_epochs = 2\n",
    "plot_nEpoch = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logging.warning('*********************************')\n",
    "    logging.warning(f'epoch: {epoch}')\n",
    "    logging.warning('*********************************')\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "    ################\n",
    "    # 1. Training\n",
    "    ################\n",
    "    \n",
    "    plot_pr_roc = (epoch % plot_nEpoch == 0)\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    # training\n",
    "    model.train_batch(train_data, optimizer, criterion)\n",
    "    \n",
    "    # acc & loss\n",
    "    train_loss, t_class_probs, t_class_label = model.val_batch(train_data, criterion)\n",
    "    val_loss, v_class_probs, v_class_label = model.val_batch(test_data, criterion)\n",
    "    # val_accs.append(val_acc)\n",
    "    \n",
    "    print('Training: Loss:')\n",
    "    print(train_loss)\n",
    "    \n",
    "    print('VAL: Loss:')\n",
    "    print(val_loss)\n",
    "    \n",
    "    logging.warning('Training:')\n",
    "    logging.warning(f'Loss: {train_loss}')\n",
    "\n",
    "    logging.warning('Validation:')\n",
    "    logging.warning(f'Loss: {val_loss}')\n",
    "    \n",
    "    ##\n",
    "    # for tensorboard plots\n",
    "    ##\n",
    "    writer.add_scalar(\"TRAIN: loss\", train_loss, epoch)\n",
    "    #writer.add_scalar(\"TRAIN: acc\", train_acc, epoch)\n",
    "    \n",
    "    writer.add_scalar(\"VAL: loss\", val_loss, epoch)\n",
    "    #writer.add_scalar(\"VAL: acc\", val_acc, epoch)\n",
    "    \n",
    "    ##\n",
    "    # PR-curve & ROC-curve\n",
    "    ##\n",
    "    if plot_pr_roc:\n",
    "        train_probs = torch.cat([batch for batch in t_class_probs])\n",
    "        train_label = torch.cat(t_class_label)\n",
    "\n",
    "        writer.add_pr_curve(f'TRAIN: pr_curve e{epoch}', train_label, train_probs, 0)\n",
    "\n",
    "        val_probs = torch.cat([batch for batch in v_class_probs])\n",
    "        val_label = torch.cat(v_class_label)\n",
    "\n",
    "        writer.add_pr_curve(f'VAL: pr_curve e{epoch}', val_label, val_probs, 0)\n",
    "\n",
    "        roc_fig = rocPlot(train_label, train_probs, val_label, val_probs)\n",
    "        writer.add_figure(f'Train vs VAL: roc_curve e{epoch}', roc_fig)\n",
    "    ##\n",
    "    # Save model every m epochs\n",
    "    ##\n",
    "    if epoch % checkpoint_m == 0:\n",
    "        cPATH = f\"checkpoint/{model_name}_{epoch}.pth\"\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': train_loss,\n",
    "                    }, cPATH)\n",
    "        \n",
    "logging.warning(f\"model_name: {model_name} \\n \\\n",
    "                batch_size: {batch_size} \\n \\\n",
    "                num_epochs: {num_epochs} \\n \\\n",
    "                learning_rate: {learning_rate} \\n \")\n",
    "                # best_val_epoch: {int(np.argmax(val_accs)+1)}\") # the best val epoch\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 1, 997, 1]             316\n",
      "            Conv2d-2            [-1, 1, 997, 1]              16\n",
      "            Conv1d-3               [-1, 1, 997]               2\n",
      "           Sigmoid-4               [-1, 1, 997]               0\n",
      "================================================================\n",
      "Total params: 334\n",
      "Trainable params: 334\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "seq_len = 997\n",
    "summary(model, (1, seq_len, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y.max(1) returns two tensors…\n",
    "\n",
    "**1. the max value in each row of y** \\\n",
    " 0.7822\\\n",
    " 0.5563\\\n",
    " 0.9425\\\n",
    "**2. the column index at which the max value is found.**\\\n",
    " 3\\\n",
    " 3\\\n",
    " 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mPATH = '/home/dimeng/code_Di/python_code/test_model.pth'\n",
    "torch.save(model.state_dict(), mPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cPATH = '/home/dimeng/code_Di/python_code/test_checkpoint.pth'\n",
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            ...\n",
    "            }, cPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(input_size, in_channels, kernel_size_row, \n",
    "                 kernel_size_col, num_classes)\n",
    "model.load_state_dict(torch.load(mPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CNN(\n",
       "  (conv1): Conv2d(1, 21, kernel_size=(7, 21), stride=(1, 1), padding=(3, 0))\n",
       "  (conv2): Conv1d(21, 21, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (conv3): Conv1d(21, 2, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       ")>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
